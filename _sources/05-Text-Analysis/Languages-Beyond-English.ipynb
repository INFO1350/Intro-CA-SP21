{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working in Languages Beyond English\n",
    "\n",
    "By [Quinn Dombrowski](http://www.quinndombrowski.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\" name=\"html-admonition\" style=\"background: lightgreen; padding: 10px\">\n",
    "<p class=\"title\">Note</p>\n",
    "This section, \"Working in Languages Beyond English,\" is authored by Quinn Dombrowski. I'm grateful to Quinn for helping expand this textbook to serve languages beyond English. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the tools and tutorials you'll find for computational text analysis assume that you're working with English text. This section is dedicated to helping students and scholars accomplish text analysis tasks in languages beyond English, demonstrating how to adapt this chapter's material for non-English languages.\n",
    "\n",
    "The steps you need to take to analyze a language beyond English will depend on the kind of text analysis method that you are interesting in using. The methods introduced in this chapter can be broadly organized into two groups: 1) methods based on word counts (e.g., TF-IDF, topic modeling) 2) and methods that use language-specific NLP algorithms (e.g., Named Entity Recognition). There are more resources to support non-English text analysis in the first group of methods than in the second group.\n",
    "\n",
    "To apply the first group of methods to non-English texts, you will need to *pre-process* your texts — in other words, to create a derivative version of your text that will work better with these tools. To apply the second group of methods to non-English texts, you will need to find a language-specific version of the NLP models. Unfortunately, for most of the roughly 6,500 languages spoken in the world, there are currently few if any language-specific tools or resources to support computational analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Non-English Texts\n",
    "\n",
    "The pre-processing steps needed to make texts in other languages usable with computational text analysis methods vary depending on the language. For example, some languages, such as Chinese, do not separate words with spaces, and texts in these languages will need to have artifical spaces inserted before text analysis.\n",
    "\n",
    "Other languages with more *inflection* than English (e.g. where words appear in different forms, depending on how they're used) need to be *lemmatized*, replacing every variant word form with the dictionary form, or *stemmed*, cutting off the inflection at the end of the word. Lemmatizing or stemming usually (but not always) leaves you with something resembling the root. For example, in Spanish `hablar` (\"to speak\") and its inflected forms `hablo` (\"I speak\") and `hablas` (\"you speak\") all become `hab` when stemmed.\n",
    "\n",
    "The situation is even more complicated for languages known as *agglutinative languages*, in which words are formed by repeatedly gluing together *morphemes*, or small bits of meaning. In agglutinative languages, a single \"word\" can be translated as an entire English sentence. How would you reduce a word like Turkish *Çekoslovakyalılaştıramadıklarımızdanmışsınız* — meaning, \"you are reportedly one of those that we could not make Czechoslovakian\" — down to a root that you could count?\n",
    "\n",
    "When doing text analysis in English, you can do things like word frequency without thinking too much about questions like \"what, actually, is a word?\" However, the ways you have to modify text in many other languages to make it compatible with computational text analysis — even to the point of harming human readability — mean that you have to grapple with this question more directly when working with other languages.\n",
    "\n",
    "Depending on what language you're working with, you may quickly discover the difference in resources available for your language, compared to English. T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-English NLP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
